# VM Benchmarks

VMB is a tool for running benchmarks for variety of Virtual Machines,
platforms and languages.

## Quick start

Prerequisites are `python3` (3.7+) with `jinja2` module installed.
(`make vmb` will install missed modules).
Virtual Machine to test should be installed on host and/or device.
See [Notes on setup for platforms](#platforms).

#### Option 1: Using wrapper script

This option is to try VMB without any installation involved.

```shell
# Run all js and ts examples on Node (needs node and tsc):
./run-vmb.sh all -p node_host `pwd`/examples/benchmarks

# Run all only js examples on Node (needs node installed):
./run-vmb.sh all -l js -p node_host `pwd`/examples/benchmarks

# See all options explained:
./run-vmb.sh help

# List all available plugins:
./run-vmb.sh list
```

#### Option2: Build and install vmb module (preferred)

Wrapper script `run-vmb.sh` has all the module functionality,
but it requires absolute paths in cmdline.
Python package, built and installed, provides more flexibility
in dealing with various benchmark sources and repos.
Once installed it exposes single `vmb` command for generating,
running and reporting tests.

```sh
# This command will build and install VMB python module on your machine.
# No root permissions required.
# After that `vmb` cli will appear in your $PATH (if not, please try adding $HOME/.local/bin)

make vmb

# Run all js and ts tests in current dir
vmb all -p node_host

# Run only js tests with tag 'sanity' from 'examples'
vmb all -p node_host -l js -T sanity ./examples
```

#### Usage Example: Compare 2 runs

```shell
export PANDA_BUILD=~/arkcompiler/runtime_core/static_core/build
# Run ets and ts tests on ArkTS in
# 1) interpretation mode
vmb all -p arkts_host --aot-skip-libs \
    --mode=int --report-json=int.json ./examples/benchmarks/
# 2) in JIT mode
vmb all -p arkts_host --aot-skip-libs \
    --mode=jit --report-json=jit.json ./examples/benchmarks/
# Compare results
vmb report --compare int.json jit.json

    Comparison: arkts_host-int vs arkts_host-jit
    ========================================
    Time: 1.99e+05->3.80e+04(better -80.9%); Size: 9.17e+04->9.17e+04(same); RSS: 4.94e+04->5.34e+04(worse +8.1%)
    =============================================================================================================
```

#### Usage example: Re-run failed tests

```shell
# provide option to save lists of failed tests:
vmb all -p node_host --fail-list ./failures.lst <path-to-test-src>
# this will produce two files:
# failures.lst - list file with paths to generate failed tests from
# failures.txt - filter test list to use with --test-list
# Re-run only failed tests:
vmb all -p node_host -fi 1 --test-list=./failures.txt ./failures.lst
```

## Commands:
`vmb` cli supports following commands:
- `gen` - generate benchmark tests from [doclets](#doclet-format)
- `run` - run tests generated by `gen` or [non doclet tests](#non-doclet-tests)
- `all` = `gen` + `run`
- `report` - works with json report (display, compare)
- `list` - show info supported langs and platforms
- `help` - print options for all the commands. (Also `vmb <cmd> --help`)

## Selecting platform
Required `-p` (`--platform`) option to `run` or `all` command should be the name
of plugin from `<vmb-package-root>/plugins/platforms`
or from `<extra-plugins-dir>/platforms` if specified.
F.e. `ark_js_vm_host` - for ArkHz on host machine,
or `arkts_ohos` - for ArkTS on OHOS device

## Selecting language and source files
`gen` command requires `-l` (`--langs`) option.
F.e. `vmb gen -l ets,swift,ts,js ./examples/benchmarks`
will generate benches for all 4 languages in examples.

Then provided to `all` command `--langs` will override langs, supported by platform.
F.e. `vmb all -l js -p v_8_host ./tests` will skip `ts` (typescript)

Source files (doclets) could be overridden
by `--src-langs` option to `all` or `run` command.

Each platform defines languages it can deal with,
and each language defines its source file extension.
Defaults are:

| platform      | langs      | sources                     |
|---------------|------------|-----------------------------|
| `arkts_*`     | `ets`      | `*.ets`                     |
| `ark_js_vm_*` | `ts`       | `*.ts`                      |
| `swift_*`     | `swift`    | `*.swift`                   |
| `v_8_*`       | `ts`, `js` | `*.ts`, `*.js`              |
| `node_*`      | `ts`, `js` | `*.ts`, `*.js`              |
| `interop_s2d` | `ets`      | `*.ets` + `lib*.*`          |
| `interop_d2s` | `ts`, `js` | `*.ts`, `*.js` + `lib*.ets` |
| `interop_d2d` | `ts`, `js` | `*.ts`, `*.js` + `lib*.*`   |
| `hap_s2*`     | `ets`      | `*.ets` + `lib*.*`          |
| `hap_d2*`     | `ts`       | `*.ts` + `lib*.*`           |

## Selecting and filtering tests:
- Any positional argument to `all` or `gen` command would be treated
as path to doclets: `vmb all -p x ./test1/test1.js ./more-tests/ ./even-more`
- Any positional argument to `run` command would be treated
as path to generated tests: `vmb run -p x ./generated`
- Files with `.lst` extension would be treated as lists of paths, relative to `CWD`
- `--tags=sanity,my` (`-T`) will generate and run only tests tagged with `sanity` OR `my`
- `--tests=Test1,'Foo_.*'` (`-t`) will run tests with names matching one of the pattern provided
- `--test-list=/path/to/list.txt` will do the same as `-t`
but reading names and/or name patterns from file (line by line)
- To exclude some tests from run point `--exclude-list` to the file with test names to exclude
- To exclude tests with tags use `-ST` (`--skip-tags`) option. F.e. `--skip-tags=negative,flaky`

Note: `--tests` option expects comma-separated list of exact test names
or regular expression (using python syntax), f.e.:
- `-t foo,bar` will select only `foo` and `bar`
- `-t '[^f]oo',foo` will select both `foo` and `boo` (selection by OR condition)
- `--tests='.*foo(_\d+)?',bar` will select `Test_foo_1 ; foo ; bar` but skip `Test_bar ; bar_1 ; foo_test`

Note: options `--tests` and `--test-list` could be combined (by OR condition) in one command line

## Benchmark's measurement options:
* `-wi` (`--warmup-iters`) controls the number of warmup iterations,
  default is 2.
* `-mi` (`--measure-iters`) controls the number of measurement iterations,
  default is 3.
* `-it` (`--iter-time`) controls the duration of iterations in seconds,
  default is 1.
* `-wt` (`--warmup-time`) controls the duration of warmup iterations in seconds,
  default is 1.
* `-gc` (`--sys-gc-pause`) Non-negative value forces GC (twice)
  and <value> milliseconds sleep before each iteration
  (GC finish can't be guaranteed on all VM's), default is -1 (no forced GC).
* `-fi` (`--fast-iters`) Number of 'fast' iterations
  (no warmup, no tuning cycles).
  Benchmark will run this number of iterations, regardless of time elapsed.

## Custom options
To provide additional option to compiler or virtual machine
`--<tool>-custom-option` could be used. F.e. add cpu profiling for `node`:
`vmb all -p node_host --node-custom-option='--cpu-prof' -v debug ./examples/benchmarks/ts/VoidBench.ts`


## Reports:
* `--report-json=path.json` to save results in json format,
   this is most detailed report.
  `--report-json-compact` disables prettifying of json.
* `--report-csv=path.csv` to save results in csv format. Only basic info included.

Apart from saved report files text report will always be printed in console.
Also json files could be displayed in separate command:
`vmb report <options> report.json`
For full list of options issue: `vmb report --help`

Note that in saved reports time (or speed) values are _nanoseconds_ (or ns/operation)
and sizes are bytes, while in console output time comes in _seconds_.

Output format could be controlled by `--number-format` option to `run|all|report` command:
- `auto` (default) : `1Âµ 1m 1K ..`
- `nano`: `1n 1000n ..`
- `expo`: `7.00e-02 5.01e+03 ..`

### Comparison of full test time

Two runs could be compared test by test on total bench time (including compilation, device interactions etc):

```shell
vmb report --compare-meta --tolerance 10 ./{1,2}.json

Full test time comparison (seconds)
===================================
name             | r1 | r2 |
============================
Sample_testSum   |  18|  18|1.77e+01->1.83e+01(same)
VoidBench_test   |   9|  11|9.21e+00->1.13e+01(worse +22.6%)

```

## Log and output:

### Log level
There are several log levels which could be set via `--log-level` option.
- `fatal`: only critical errors will be printed
- `pass`: print single line for each test after it finishes
- `error`
- `warn`
- `info`: default level
- `debug`
- `trace`: most verbose

### Print test list
`vmb gen -l ts --show-list ./examples/benchmarks`

### Produce lists for re-run failed tests
`vmb all -p node_host --fail-list ./failures.lst ./examples/benchmarks`

Each level prints in its own color.
`--no-color` disables color and adds `[%LEVEL]` prefix to each message.

## Extra (custom) plugins:

Providing option `--extra-plugins` you can add any
language, tool, platform or hook.
This parameter should point to the directory containing any combination of
`hooks`, `langs`, `platforms`, `tools` sub-dirs.
As the simplest case you can copy any of the existing plugins
and safely experiment modifying it.
`extra-plugins` has higher 'priority', so any tools, platforms and langs
could be 'overridden' by custom ones.

```sh
# Example:
# 2 demo plugins: Tool=dummy and Platform=dummy_host
mkdir bu_Fake
touch bu_Fake/bench_Fake.txt
vmb run -p dummy_host \
    --extra-plugins=./examples/plugins \
    ./bu_Fake/bench_Fake.txt
```

## Doclet format:
This is a mean for benchmark declaration in source file.
In a single root class it is possible to define any number of benchmarks,
setup method, benchmarks parameters, run options and some additional meta-info.
See examples in `examples/benchamrks` directory.

Using this test format involves separate generation stage (`gen` command).
Resulting benchmark programs will be generated using templates
in `plugins/templates` into `generated` directory
(which could be overriden by `--outdir` option)

Supported doclets are:
* `@State` on a root class which contains benchmarks tests as its methods.
* `@Setup` on an initialization method of a state.
* `@Benchmark` on a method that is measured.
   It should not accept any parameters and
   (preferably) it may return a value which is consumed.
* `@Param  p1 [, p2...]` on a state field.
   Attribute define values to create several benchmarks using same code,
   and all combinations of params.
   Value can be an int, a string or a comma separated list of ints or strings.
* `@Import {x, y...} from ./libX.ext` produces import statement and compile `libX.ext` if needed.
* `@Include ./f.ext` paste contents of `f.ext` into generated source.
* `@Tags t1 [, t2...]` on a root class or method. List of benchmark tags.
* `@Bugs b1 [, b2...]` on a root class or method. List of associated issues.

JS specific doclets (JSDoc-like) are:
* `@arg {<State name>} <name>` on a benchmark method.
   Links a proper state class to each parameter.
* `@returns {Bool|Char|Int|Float|Obj|Objs}` optionally on a benchmark method.
   Helps to build automatic `Consumer` usage.

## Non-doclet tests

It is possible to write 'free-style' tests which can be ran as is.
VMB will search bench units in directories with `bu_` prefix and files with
`bench_` prefix.
Test result should be reported by program
as `Benchmark result: <TestName> <time in ns>`.

## Platforms

### ArkTS (arkts_*)
On host: env var `PANDA_BUILD` should be set to point to `build` directory
f.e. `~/arkcompiler/runtime_core/static_core/build`

On device: `ark` and `ark_aot` binaries with required libraries should be pushed to device.
Default place is `/data/local/tmp/vmb` and could be configured via `--device-dir`

By default `etsstdlib` will be compiled into native code,
to avoid this use `--aot-skip-libs` (`-A`) option

### Ark JS VM (ark_js_vm_*)
On host: env var `OHOS_SDK` sould be set to point to OHOS SDK source directory
inside which `out/*.release` should contain built binaries

On device: `ark_js_vm` and `ark_aot_compiler` should be pushed to device.
Default place is `/data/local/tmp/vmb` and could be configured via `--device-dir`

### V_8
For device platforms: required binaries should be pushed to device.
Default place is `/data/local/tmp/vmb/v_8`.
`/data/local/tmp/vmb` part could be configured via `--device-dir`

### Hap (Ability Package) - Experimental
This platform allows to run `arkts/ets` benchmarks as application ability

##### Prerequisites:
- `PANDA_SDK` env var should point to unpacked Panda OHOS SDK (package dir)
- `OHOS_BASE_SDK_HOME` env var should point to unpacked OHOS SDK
- `hvigorw` should be in PATH or `HVIGORW` env var should point to hvigor script or binary
- `hdc` should be in PATH or `HDC` env var should point to hdc script or binary
- For signing package `HAP_SIGNING_CONFIG` env var should point to json config 
    as in `app/signingConfigs/material` section of `build-profile.json5`

##### Run
```sh
vmb all -p hap -A examples/benchmarks/ets

# --tests-per-batch option could be used to tune amount of benchmarks per one hap package (25 is the default)
```
##### Limitations
- Total test run inside app is limited to 5 sec, so `-wi` is limited to 0..1 and `-mi` to 1..2.
    `-wt` and `it` has no effect for `hap` platform.
- "Macro" benchmarks (i.e. test functions which run longer than 1 sec) won't produce benchmark result.


## Platform Features

| platform       | int-mode | aot-mode | jit-mode | gc-stats | jit-stats | aot-stats | imports |
|----------------|:--------:|:--------:|:--------:|:--------:|:---------:|:---------:|:-------:|
| ark_js_vm_host |   n/a    |    V     |    V     |   n/a    |    n/a    |    n/a    |    V    |
| ark_js_vm_ohos |   n/a    |    V     |    V     |   n/a    |    n/a    |    n/a    |    V    |
| arkts_device   |    V     |    V     |    V     |    V     |     V     |     V     |    V    |
| arkts_host     |    V     |    V     |    V     |    V     |     V     |     V     |    V    |
| arkts_ohos     |    V     |    V     |    V     |    V     |     V     |     V     |    V    |
| node_host      |   n/a    |   n/a    |    V     |   n/a    |    n/a    |    n/a    |    V    |
| swift_device   |   n/a    |   n/a    |   n/a    |   n/a    |    n/a    |    n/a    |    X    |
| swift_host     |   n/a    |   n/a    |   n/a    |   n/a    |    n/a    |    n/a    |    X    |
| v_8_device     |    V     |   n/a    |    V     |   n/a    |    n/a    |    n/a    |    V    |
| v_8_host       |    V     |   n/a    |    V     |   n/a    |    n/a    |    n/a    |    V    |
| v_8_ohos       |    V     |   n/a    |    V     |   n/a    |    n/a    |    n/a    |    V    |
| interop_d2s    |    V     |   n/a    |   n/a    |   n/a    |    n/a    |    n/a    |    V    |
| interop_s2d    |    V     |   n/a    |   n/a    |   n/a    |    n/a    |    n/a    |    V    |
| interop_d2d    |    V     |   n/a    |   n/a    |   n/a    |    n/a    |    n/a    |    V    |
| hap_d2s        |    V     |    X     |    X     |   n/a    |    n/a    |    n/a    |    V    |
| hap_s2d        |    V     |    X     |    X     |   n/a    |    n/a    |    n/a    |    V    |
| hap_d2d        |    V     |    X     |    X     |   n/a    |    n/a    |    n/a    |    V    |
| hap_s2s        |    V     |    X     |    X     |   n/a    |    n/a    |    n/a    |    V    |

## Interoperability tests:

Please refer to [this manual](./examples/benchmarks/interop/readme.md) and [examples](./examples/benchmarks/interop).

See also [readme here](../../plugins/ets/tests/benchmarks/interop_js/README.md)

## Self tests and linters

To run all unit tests and linters ussue following command:

```sh
make tox
```
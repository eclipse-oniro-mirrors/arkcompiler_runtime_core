# plugin typed_arrays
# Copyright (c) 2024 Huawei Device Co., Ltd.
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

include_relative 'common.irt'
include_relative '../../../irtoc/scripts/common.irt'
include_relative '../../../irtoc/scripts/string_helpers.irt'

module Constants
  ARRAY_BUFFER_CLASS_SIZE = "cross_values::GetArrayBufferClassSize(GetArch())"
  ARRAY_BUFFER_BYTE_LENGTH_OFFSET =  "cross_values::GetArrayBufferByteLengthOffset(GetArch())"
  ARRAY_BUFFER_DATA_OFFSET = "cross_values::GetArrayBufferDataOffset(GetArch())"
  ARRAY_BUFFER_NATIVE_DATA_OFFSET = "cross_values::GetArrayBufferNativeDataOffset(GetArch())"
  ARRAY_BUFFER_MANAGED_DATA_OFFSET = "cross_values::GetArrayBufferManagedDataOffset(GetArch())"
  ARRAY_BUFFER_IS_RESIZABLE_OFFSET = "cross_values::GetArrayBufferIsResizableOffset(GetArch())"
  ARRAY_BUFFER_CLASS_SIZE_WITH_ALIGNMENT = "cross_values::GetArrayBufferClassSize(GetArch()) + TLAB_ALIGNMENT - 1"

  TYPED_UNSIGNED_ARRAY_BYTES_PER_ELEMENT_OFFSET = "cross_values::GetTypedUnsignedArrayBytesPerElementOffset(GetArch())"
  TYPED_UNSIGNED_ARRAY_BYTE_OFFSET_OFFSET = "cross_values::GetTypedUnsignedArrayByteOffsetOffset(GetArch())"
  TYPED_UNSIGNED_ARRAY_BYTE_LENGTH_OFFSET = "cross_values::GetTypedUnsignedArrayByteLengthOffset(GetArch())"
  TYPED_UNSIGNED_ARRAY_LENGTH_INT_OFFSET = "cross_values::GetTypedUnsignedArrayLengthIntOffset(GetArch())"
  TYPED_UNSIGNED_ARRAY_LENGTH_OFFSET = "cross_values::GetTypedUnsignedArrayLengthOffset(GetArch())"
  TYPED_UNSIGNED_ARRAY_CLASS_SIZE = "cross_values::GetTypedUnsignedArrayClassSize(GetArch())"
  TYPED_UNSIGNED_ARRAY_BUFFER_OFFSET = "cross_values::GetTypedUnsignedArrayBufferOffset(GetArch())"
  TYPED_UNSIGNED_ARRAY_CLASS_SIZE_WITH_ALIGNMENT = "cross_values::GetTypedUnsignedArrayClassSize(GetArch()) + TLAB_ALIGNMENT - 1"

  TYPED_ARRAY_BYTES_PER_ELEMENT_OFFSET = "cross_values::GetTypedArrayBytesPerElementOffset(GetArch())"
  TYPED_ARRAY_BYTE_OFFSET_OFFSET = "cross_values::GetTypedArrayByteOffsetOffset(GetArch())"
  TYPED_ARRAY_BYTE_LENGTH_OFFSET = "cross_values::GetTypedArrayByteLengthOffset(GetArch())"
  TYPED_ARRAY_LENGTH_INT_OFFSET = "cross_values::GetTypedArrayLengthIntOffset(GetArch())"
  TYPED_ARRAY_LENGTH_OFFSET = "cross_values::GetTypedArrayLengthOffset(GetArch())"
  TYPED_ARRAY_CLASS_SIZE = "cross_values::GetTypedArrayClassSize(GetArch())"
  TYPED_ARRAY_BUFFER_OFFSET = "cross_values::GetTypedArrayBufferOffset(GetArch())"
  TYPED_ARRAY_CLASS_SIZE_WITH_ALIGNMENT = "cross_values::GetTypedArrayClassSize(GetArch()) + TLAB_ALIGNMENT - 1"
end

def GenerateTypedArrayFillInternal(name, prefix, type, scale)
    function("#{name}ArrayFillInternalFastPath".to_sym,
            params: {arr: 'ref', val: type, startPos: 'i32', endPos: 'i32'},
            regmap: $full_regmap,
            regalloc_set: $panda_mask,
            mode: [:FastPath]) {

        if Options.arch == :arm32
            Intrinsic(:UNREACHABLE).Terminator.void
            next
        end
        buffer := LoadI(arr).Imm(Constants::TYPED_ARRAY_BUFFER_OFFSET).ref
        bufferData := LoadI(buffer).Imm(Constants::ARRAY_BUFFER_DATA_OFFSET).ref
        If(bufferData, 0).EQ.Unlikely {
          Goto(:SlowPathEntrypoint)
        }
        byteOffsetF64 := LoadI(arr).Imm(Constants::TYPED_ARRAY_BYTE_OFFSET_OFFSET).f64
        byteOffset := Cast(byteOffsetF64).i32
        arrayDataOffset := AddI(byteOffset).Imm(Constants::ARRAY_DATA_OFFSET).i32
        offset0 := Add(arrayDataOffset, ShlI(startPos).Imm(scale).i32).i32
        offsetEnd := Add(arrayDataOffset, ShlI(endPos).Imm(scale).i32).i32

      Label(:Loop)
        offset := Phi(offset0, offset1).u32
        If(offset, offsetEnd).GE {
            Goto(:End)
        }
        Store(bufferData, offset, val).send(type)
        offset1 := AddI(offset).Imm(1 << scale).i32
        Goto(:Loop)

      Label(:End)
        ReturnVoid().void
      Label(:SlowPathEntrypoint)
        ep_offset = get_entrypoint_offset("#{prefix}_ARRAY_FILL_INTERNAL_USUAL")
        Intrinsic(:SLOW_PATH_ENTRY, arr, val, startPos, endPos).AddImm(ep_offset).MethodAsImm("#{name}ArrayFillInternalUsualBridge").Terminator.void
        Intrinsic(:UNREACHABLE).Terminator.void if defines.DEBUG
    }
end

GenerateTypedArrayFillInternal('Int8', 'INT8', 'i8', 0)
GenerateTypedArrayFillInternal('Int16', 'INT16', 'i16', 1)
GenerateTypedArrayFillInternal('Int32', 'INT32', 'i32', 2)
GenerateTypedArrayFillInternal('BigInt64', 'BIG_INT64', 'i64', 3)
GenerateTypedArrayFillInternal('Float32', 'FLOAT32', 'i32', 2)
GenerateTypedArrayFillInternal('Float64', 'FLOAT64', 'i64', 3)

def GenerateTypedUnsignedArrayFillInternal(name, prefix, inType, outType, scale)
    function("#{name}ArrayFillInternalFastPath".to_sym,
            params: {arr: 'ref', val: inType, startPos: 'i32', endPos: 'i32'},
            regmap: $full_regmap,
            regalloc_set: $panda_mask,
            mode: [:FastPath]) {

        if Options.arch == :arm32
            Intrinsic(:UNREACHABLE).Terminator.void
            next
        end
        buffer := LoadI(arr).Imm(Constants::TYPED_UNSIGNED_ARRAY_BUFFER_OFFSET).ref
        bufferData := LoadI(buffer).Imm(Constants::ARRAY_BUFFER_DATA_OFFSET).ref
        If(bufferData, 0).EQ.Unlikely {
          Goto(:SlowPathEntrypoint)
        }
        byteOffset := LoadI(arr).Imm(Constants::TYPED_ARRAY_BYTE_OFFSET_OFFSET).i32
        arrayDataOffset := AddI(byteOffset).Imm(Constants::ARRAY_DATA_OFFSET).i32
        offset0 := Add(arrayDataOffset, ShlI(startPos).Imm(scale).i32).i32
        offsetEnd := Add(arrayDataOffset, ShlI(endPos).Imm(scale).i32).i32

      Label(:Loop)
        offset := Phi(offset0, offset1).u32
        If(offset, offsetEnd).GE {
            Goto(:End)
        }
        Store(bufferData, offset, val).send(outType)
        offset1 := AddI(offset).Imm(1 << scale).i32
        Goto(:Loop)

      Label(:End)
        ReturnVoid().void
      Label(:SlowPathEntrypoint)
        ep_offset = get_entrypoint_offset("#{prefix}_ARRAY_FILL_INTERNAL_USUAL")
        Intrinsic(:SLOW_PATH_ENTRY, arr, val, startPos, endPos).AddImm(ep_offset).MethodAsImm("#{name}ArrayFillInternalUsualBridge").Terminator.void
        Intrinsic(:UNREACHABLE).Terminator.void if defines.DEBUG
    }
end

GenerateTypedUnsignedArrayFillInternal('UInt8Clamped', 'U_INT8', 'i32', 'u8', 0)
GenerateTypedUnsignedArrayFillInternal('UInt8', 'U_INT8', 'i32', 'u8', 0)
GenerateTypedUnsignedArrayFillInternal('UInt16', 'U_INT16', 'i32', 'u16', 1)
GenerateTypedUnsignedArrayFillInternal('UInt32', 'U_INT32', 'i64', 'u32', 2)
GenerateTypedUnsignedArrayFillInternal('BigUInt64', 'BIG_U_INT64', 'i64', 'u64', 3)

# Try to allocate object in TLAB.
# The result is either a pointer to a new object or null if there is no enough space in TLAB.
macro(:allocate_object_tlab) do |klass, klass_size, data_size|
    if Options.arch == :arm32
      Intrinsic(:UNREACHABLE).Terminator.void
      ReturnVoid().void
      next
    end

    _data_size := Cast(data_size).word
    _klass_size_align := AddI(Cast(klass_size).word).Imm("TLAB_ALIGNMENT - 1").word
    _size_0 := AndI(_klass_size_align).Imm(Constants::ALIGNMENT_MASK).word
    If(_data_size, Cast(0).word).NE.Unlikely {
        _size_1 := Add(Add(_data_size, Cast(klass_size).word).word, "DEFAULT_ALIGNMENT_IN_BYTES - 1").word
        _size_1 := And(_size_1, "(~(DEFAULT_ALIGNMENT_IN_BYTES - 1))").word
    }
    _size := Phi(_size_0, _size_1).word

    # Load pointer to the TLAB from TLS
    _tlab := LoadI(%tr).Imm(Constants::TLAB_OFFSET).ptr
    # Load pointer to the start address of free memory in the TLAB
    _start := LoadI(_tlab).Imm(Constants::TLAB_CUR_FREE_POSITION_OFFSET).ptr
    # Load pointer to the end address of free memory in the TLAB
    _end := LoadI(_tlab).Imm(Constants::TLAB_MEMORY_END_ADDR_OFFSET).ptr
    # Check if there is enough space
    If(Sub(_end, _start).word, _size).B.Unlikely {
        Goto(:SlowPathEntrypoint)
    }
    Intrinsic(:WRITE_TLAB_STATS_SAFE, _start, _size, Cast(-1).u64).void if defines.DEBUG
    if defines.__SANITIZE_ADDRESS__ || defines.__SANITIZE_THREAD__
      call_runtime_save_all(Constants::ANNOTATE_SANITIZERS_NO_BRIDGE, _start, _size).void
    end
    # Store class of the object
    StoreI(_start, klass).Imm(Constants::OBJECT_CLASS_OFFSET).ref
    # Update the TLAB state
    StoreI(Add(_tlab, Constants::TLAB_CUR_FREE_POSITION_OFFSET).ptr, Add(_start, _size).ptr).Imm(0).Volatile.ptr
    # Return a pointer to the newly allocated string
    _allocated_object := _start
end

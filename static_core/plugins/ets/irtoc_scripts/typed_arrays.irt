# plugin typed_arrays
# Copyright (c) 2024 Huawei Device Co., Ltd.
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

include_relative 'common.irt'
include_relative '../../../irtoc/scripts/common.irt'
include_relative '../../../irtoc/scripts/string_helpers.irt'

module Constants
  ARRAY_BUFFER_CLASS_SIZE = "cross_values::GetArrayBufferClassSize(GetArch())"
  ARRAY_BUFFER_BYTE_LENGTH_OFFSET =  "cross_values::GetArrayBufferByteLengthOffset(GetArch())"
  ARRAY_BUFFER_DATA_OFFSET = "cross_values::GetArrayBufferDataOffset(GetArch())"
  ARRAY_BUFFER_NATIVE_DATA_OFFSET = "cross_values::GetArrayBufferNativeDataOffset(GetArch())"
  ARRAY_BUFFER_MANAGED_DATA_OFFSET = "cross_values::GetArrayBufferManagedDataOffset(GetArch())"
  ARRAY_BUFFER_IS_RESIZABLE_OFFSET = "cross_values::GetArrayBufferIsResizableOffset(GetArch())"
  ARRAY_BUFFER_CLASS_SIZE_WITH_ALIGNMENT = "cross_values::GetArrayBufferClassSize(GetArch()) + TLAB_ALIGNMENT - 1"

  TYPED_UNSIGNED_ARRAY_BYTES_PER_ELEMENT_OFFSET = "cross_values::GetTypedUnsignedArrayBytesPerElementOffset(GetArch())"
  TYPED_UNSIGNED_ARRAY_BYTE_OFFSET_OFFSET = "cross_values::GetTypedUnsignedArrayByteOffsetOffset(GetArch())"
  TYPED_UNSIGNED_ARRAY_BYTE_LENGTH_OFFSET = "cross_values::GetTypedUnsignedArrayByteLengthOffset(GetArch())"
  TYPED_UNSIGNED_ARRAY_LENGTH_INT_OFFSET = "cross_values::GetTypedUnsignedArrayLengthIntOffset(GetArch())"
  TYPED_UNSIGNED_ARRAY_CLASS_SIZE = "cross_values::GetTypedUnsignedArrayClassSize(GetArch())"
  TYPED_UNSIGNED_ARRAY_BUFFER_OFFSET = "cross_values::GetTypedUnsignedArrayBufferOffset(GetArch())"
  TYPED_UNSIGNED_ARRAY_CLASS_SIZE_WITH_ALIGNMENT = "cross_values::GetTypedUnsignedArrayClassSize(GetArch()) + TLAB_ALIGNMENT - 1"

  TYPED_ARRAY_BYTES_PER_ELEMENT_OFFSET = "cross_values::GetTypedArrayBytesPerElementOffset(GetArch())"
  TYPED_ARRAY_BYTE_OFFSET_OFFSET = "cross_values::GetTypedArrayByteOffsetOffset(GetArch())"
  TYPED_ARRAY_BYTE_LENGTH_OFFSET = "cross_values::GetTypedArrayByteLengthOffset(GetArch())"
  TYPED_ARRAY_LENGTH_INT_OFFSET = "cross_values::GetTypedArrayLengthIntOffset(GetArch())"
  TYPED_ARRAY_CLASS_SIZE = "cross_values::GetTypedArrayClassSize(GetArch())"
  TYPED_ARRAY_BUFFER_OFFSET = "cross_values::GetTypedArrayBufferOffset(GetArch())"
  TYPED_ARRAY_CLASS_SIZE_WITH_ALIGNMENT = "cross_values::GetTypedArrayClassSize(GetArch()) + TLAB_ALIGNMENT - 1"
end


def GenerateTypedArrayFillInternal(name, prefix, type, scale)
    function("#{name}ArrayFillInternalFastPath".to_sym,
            params: {arr: 'ref', val: type, startPos: 'i32', endPos: 'i32'},
            regmap: $full_regmap,
            regalloc_set: $panda_mask,
            mode: [:FastPath]) {

        if Options.arch == :arm32
            Intrinsic(:UNREACHABLE).Terminator.void
            next
        end
        buffer := LoadI(arr).Imm(Constants::TYPED_ARRAY_BUFFER_OFFSET).ref
        bufferData := LoadI(buffer).Imm(Constants::ARRAY_BUFFER_DATA_OFFSET).ref
        If(bufferData, 0).EQ.Unlikely {
          Goto(:SlowPathEntrypoint)
        }
        byteOffsetF64 := LoadI(arr).Imm(Constants::TYPED_ARRAY_BYTE_OFFSET_OFFSET).f64
        byteOffset := Cast(byteOffsetF64).i32
        arrayDataOffset := AddI(byteOffset).Imm(Constants::ARRAY_DATA_OFFSET).i32
        offset0 := Add(arrayDataOffset, ShlI(startPos).Imm(scale).i32).i32
        offsetEnd := Add(arrayDataOffset, ShlI(endPos).Imm(scale).i32).i32

      Label(:Loop)
        offset := Phi(offset0, offset1).u32
        If(offset, offsetEnd).GE {
            Goto(:End)
        }
        Store(bufferData, offset, val).send(type)
        offset1 := AddI(offset).Imm(1 << scale).i32
        Goto(:Loop)

      Label(:End)
        ReturnVoid().void
      Label(:SlowPathEntrypoint)
        ep_offset = get_entrypoint_offset("#{prefix}_ARRAY_FILL_INTERNAL_USUAL")
        Intrinsic(:SLOW_PATH_ENTRY, arr, val, startPos, endPos).AddImm(ep_offset).MethodAsImm("#{name}ArrayFillInternalUsualBridge").Terminator.void
        Intrinsic(:UNREACHABLE).Terminator.void if defines.DEBUG
    }
end

GenerateTypedArrayFillInternal('Int8', 'INT8', 'i8', 0)
GenerateTypedArrayFillInternal('Int16', 'INT16', 'i16', 1)
GenerateTypedArrayFillInternal('Int32', 'INT32', 'i32', 2)
GenerateTypedArrayFillInternal('BigInt64', 'BIG_INT64', 'i64', 3)
GenerateTypedArrayFillInternal('Float32', 'FLOAT32', 'i32', 2)
GenerateTypedArrayFillInternal('Float64', 'FLOAT64', 'i64', 3)

def GenerateTypedUnsignedArrayFillInternal(name, prefix, inType, outType, scale)
    function("#{name}ArrayFillInternalFastPath".to_sym,
            params: {arr: 'ref', val: inType, startPos: 'i32', endPos: 'i32'},
            regmap: $full_regmap,
            regalloc_set: $panda_mask,
            mode: [:FastPath]) {

        if Options.arch == :arm32
            Intrinsic(:UNREACHABLE).Terminator.void
            next
        end
        buffer := LoadI(arr).Imm(Constants::TYPED_UNSIGNED_ARRAY_BUFFER_OFFSET).ref
        bufferData := LoadI(buffer).Imm(Constants::ARRAY_BUFFER_DATA_OFFSET).ref
        If(bufferData, 0).EQ.Unlikely {
          Goto(:SlowPathEntrypoint)
        }
        byteOffset := LoadI(arr).Imm(Constants::TYPED_ARRAY_BYTE_OFFSET_OFFSET).i32
        arrayDataOffset := AddI(byteOffset).Imm(Constants::ARRAY_DATA_OFFSET).i32
        offset0 := Add(arrayDataOffset, ShlI(startPos).Imm(scale).i32).i32
        offsetEnd := Add(arrayDataOffset, ShlI(endPos).Imm(scale).i32).i32

      Label(:Loop)
        offset := Phi(offset0, offset1).u32
        If(offset, offsetEnd).GE {
            Goto(:End)
        }
        Store(bufferData, offset, val).send(outType)
        offset1 := AddI(offset).Imm(1 << scale).i32
        Goto(:Loop)

      Label(:End)
        ReturnVoid().void
      Label(:SlowPathEntrypoint)
        ep_offset = get_entrypoint_offset("#{prefix}_ARRAY_FILL_INTERNAL_USUAL")
        Intrinsic(:SLOW_PATH_ENTRY, arr, val, startPos, endPos).AddImm(ep_offset).MethodAsImm("#{name}ArrayFillInternalUsualBridge").Terminator.void
        Intrinsic(:UNREACHABLE).Terminator.void if defines.DEBUG
    }
end

GenerateTypedUnsignedArrayFillInternal('UInt8Clamped', 'U_INT8', 'i32', 'u8', 0)
GenerateTypedUnsignedArrayFillInternal('UInt8', 'U_INT8', 'i32', 'u8', 0)
GenerateTypedUnsignedArrayFillInternal('UInt16', 'U_INT16', 'i32', 'u16', 1)
GenerateTypedUnsignedArrayFillInternal('UInt32', 'U_INT32', 'i64', 'u32', 2)
GenerateTypedUnsignedArrayFillInternal('BigUInt64', 'BIG_U_INT64', 'i64', 'u64', 3)

# Try to allocate object in TLAB.
# The result is either a pointer to a new object or null if there is no enough space in TLAB.
macro(:allocate_object_tlab) do |klass, klass_size, data_size|
    if Options.arch == :arm32
      Intrinsic(:UNREACHABLE).Terminator.void
      ReturnVoid().void
      next
    end

    _data_size := Cast(data_size).word
    _klass_size_align := AddI(Cast(klass_size).word).Imm("TLAB_ALIGNMENT - 1").word
    _size_0 := AndI(_klass_size_align).Imm(Constants::ALIGNMENT_MASK).word
    If(_data_size, Cast(0).word).NE.Unlikely {
        _size_1 := Add(Add(_data_size, Cast(klass_size).word).word, "DEFAULT_ALIGNMENT_IN_BYTES - 1").word
        _size_1 := And(_size_1, "(~(DEFAULT_ALIGNMENT_IN_BYTES - 1))").word
    }
    _size := Phi(_size_0, _size_1).word

    # Load pointer to the TLAB from TLS
    _tlab := LoadI(%tr).Imm(Constants::TLAB_OFFSET).ptr
    # Load pointer to the start address of free memory in the TLAB
    _start := LoadI(_tlab).Imm(Constants::TLAB_CUR_FREE_POSITION_OFFSET).ptr
    # Load pointer to the end address of free memory in the TLAB
    _end := LoadI(_tlab).Imm(Constants::TLAB_MEMORY_END_ADDR_OFFSET).ptr
    # Check if there is enough space
    If(Sub(_end, _start).word, _size).B.Unlikely {
      Goto(:SlowPathEntrypoint)
    }
    Intrinsic(:WRITE_TLAB_STATS_SAFE, _start, _size, Cast(-1).u64).void if defines.DEBUG
    if defines.__SANITIZE_ADDRESS__ || defines.__SANITIZE_THREAD__
      call_runtime_save_all(Constants::ANNOTATE_SANITIZERS_NO_BRIDGE, _start, _size).void
    end
    # Store class of the object
    StoreI(_start, klass).Imm(Constants::OBJECT_CLASS_OFFSET).ref
    # Update the TLAB state
    StoreI(Add(_tlab, Constants::TLAB_CUR_FREE_POSITION_OFFSET).ptr, Add(_start, _size).ptr).Imm(0).Volatile.ptr
    # Return a pointer to the newly allocated string
    _allocated_object := _start
end


# Try to allocate TypedArray, ArrayBuffer, Array in TLAB.
# The result is a pointers to a new TypedArray, ArrayBuffer, Array or null if there is no enough space in TLAB.
macro(:allocate_tarray_tlab) do |packet_klass, buffer_klass, tarray_klass, data_size, tarray_klass_size|
  if Options.arch == :arm32
    Intrinsic(:UNREACHABLE).Terminator.void
    ReturnVoid().void
    next
  end

  # Get size
  _tarray_size := AndI(Cast(tarray_klass_size).word).Imm(Constants::ALIGNMENT_MASK).word
  _buffer_size := AndI(Cast(Constants::ARRAY_BUFFER_CLASS_SIZE_WITH_ALIGNMENT).word).Imm(Constants::ALIGNMENT_MASK).word
  _packet_size := Add(Add(Cast(data_size).word, Cast(Constants::ARRAY_CLASS_SIZE).word).word, "DEFAULT_ALIGNMENT_IN_BYTES - 1").word
  _packet_size := And(_packet_size, "(~(DEFAULT_ALIGNMENT_IN_BYTES - 1))").word
  _size = Add(Add(_tarray_size, _buffer_size).word, _packet_size).word

  # Load pointer to the TLAB from TLS
  _tlab := LoadI(%tr).Imm(Constants::TLAB_OFFSET).ptr
  # Load pointer to the start address of free memory in the TLAB (Typed Array address)
  _start := LoadI(_tlab).Imm(Constants::TLAB_CUR_FREE_POSITION_OFFSET).ptr
  # Load pointer to the end address of free memory in the TLAB
  _end := LoadI(_tlab).Imm(Constants::TLAB_MEMORY_END_ADDR_OFFSET).ptr
  # Check if there is enough space
  If(Sub(_end, _start).word, _size).B.Unlikely {
    Goto(:SlowPathEntrypoint)
  }
  Intrinsic(:WRITE_TLAB_STATS_SAFE, _start, _size, Cast(-1).u64).void if defines.DEBUG
  if defines.__SANITIZE_ADDRESS__ || defines.__SANITIZE_THREAD__
    call_runtime_save_all(Constants::ANNOTATE_SANITIZERS_NO_BRIDGE, _start, _size).void
  end

  # get ArrayBuffer, Array pointers
  _buff := Add(_start, _tarray_size).ptr
  _pack := Add(_buff, _buffer_size).ptr
  # Store classes of TypedArray, ArrayBuffer and Array
  StoreI(_start, tarray_klass).Imm(Constants::OBJECT_CLASS_OFFSET).ref
  StoreI(_buff, buffer_klass).Imm(Constants::OBJECT_CLASS_OFFSET).ref
  StoreI(_pack, packet_klass).Imm(Constants::OBJECT_CLASS_OFFSET).ref
  # Update the TLAB state
  StoreI(Add(_tlab, Constants::TLAB_CUR_FREE_POSITION_OFFSET).ptr, Add(_start, _size).ptr).Imm(0).Volatile.ptr

  # result
  new_tarray := _start
  new_buffer := _buff
  new_packet := _pack
end


def GenerateToReversed(type)
  case type
  when "Uint8"
    prefix = "TYPED_UNSIGNED"
  when "Uint16"
    prefix = "TYPED_UNSIGNED"
  when "Uint32"
    prefix = "TYPED_UNSIGNED"
  when "BigUint64"
    prefix = "TYPED_UNSIGNED"
  else
    prefix = "TYPED"
  end
  case type
    when "Int8"
      suffix = "i8"
    when "Int16"
      suffix = "i16"
    when "Int32"
      suffix = "i32"
    when "BigInt64"
      suffix = "i64"
    when "Float32"
      suffix = "f32"
    when "Float64"
      suffix = "f64"
    when "Uint8"
      suffix = "u8"
    when "Uint16"
      suffix = "u16"
    when "Uint32"
      suffix = "u32"
    when "BigUint64"
      suffix = "u64"
    else
      raise "Unexpected type: #{type}"
  end
  case type
    when "BigInt64"
      macroType = "BIG_INT64"
    when "BigUint64"
      macroType = "BIG_UINT64"
  else
    macroType = type.upcase
  end
  function("#{type}ArrayToReversedTlab".to_sym,
      params: {typed_array: 'ref'},
      regmap: $full_regmap,
      regalloc_set: $panda_mask,
      mode: [:FastPath]) {

    # not supported
    if Options.arch == :arm32
      Intrinsic(:UNREACHABLE).Terminator.void
      next
    end

    # load TypedArray params
    eval("byte_offset := Cast(LoadI(typed_array).Imm(Constants::#{prefix}_ARRAY_BYTE_OFFSET_OFFSET).f64).u32")
    eval("elm_size := LoadI(typed_array).Imm(Constants::#{prefix}_ARRAY_BYTES_PER_ELEMENT_OFFSET).u32")
    eval("arr_len := Cast(LoadI(typed_array).Imm(Constants::#{prefix}_ARRAY_LENGTH_INT_OFFSET).i32).u32")
    array_byte_len := Mul(arr_len, elm_size).u32

    # new Typed Array
    klass := LoadI(typed_array).Imm(Constants::OBJECT_CLASS_OFFSET).ref
    eval("new_typed_array := allocate_object_tlab(klass, Constants::#{prefix}_ARRAY_CLASS_SIZE, 0)")
    eval("copy_u8_chars(typed_array, new_typed_array, Constants::#{prefix}_ARRAY_CLASS_SIZE)")
    eval("StoreI(new_typed_array, Cast(0).f64).Imm(Constants::#{prefix}_ARRAY_BYTE_OFFSET_OFFSET).f64")

    # new ArrayBuffer
    eval("array_buffer := LoadI(typed_array).Imm(Constants::#{prefix}_ARRAY_BUFFER_OFFSET).ref")
    array_buffer_klass := LoadI(array_buffer).Imm(Constants::OBJECT_CLASS_OFFSET).ref
    new_array_buffer := allocate_object_tlab(array_buffer_klass, Constants::ARRAY_BUFFER_CLASS_SIZE, 0)
    eval("StoreI(new_typed_array, new_array_buffer).Imm(Constants::#{prefix}_ARRAY_BUFFER_OFFSET).ptr")

    # new Array
    array := LoadI(array_buffer).Imm(Constants::ARRAY_BUFFER_DATA_OFFSET).ref
    If(array, 0).EQ.Unlikely {
      Goto(:SlowPathEntrypoint)
    }
    array_klass := LoadI(array).Imm(Constants::OBJECT_CLASS_OFFSET).ref
    new_packet := allocate_object_tlab(array_klass, Constants::ARRAY_CLASS_SIZE, array_byte_len)
    StoreI(new_packet, array_byte_len).Imm(Constants::ARRAY_LENGTH_OFFSET).u32
    src_data := Add(AddI(array).Imm(Constants::ARRAY_DATA_OFFSET).ptr, byte_offset).ptr
    dst_data := AddI(new_packet).Imm(Constants::ARRAY_DATA_OFFSET).ptr
    copy_u8_chars(src_data, dst_data, array_byte_len)

    StoreI(new_array_buffer, new_packet).Imm(Constants::ARRAY_BUFFER_DATA_OFFSET).ptr
    StoreI(new_array_buffer, dst_data).Imm(Constants::ARRAY_BUFFER_NATIVE_DATA_OFFSET).ptr
    StoreI(new_array_buffer, array_byte_len).Imm(Constants::ARRAY_BUFFER_BYTE_LENGTH_OFFSET).u32
    StoreI(new_array_buffer, Cast(0).u8).Imm(Constants::ARRAY_BUFFER_IS_RESIZABLE_OFFSET).u8

    # fill new buffer in reversed order
    src_ofs_0 := Sub(array_byte_len, elm_size).u32
    dst_ofs_0 := 0
  Label(:CopyLoop)
    src_ofs := Phi(src_ofs_0, src_ofs_1).u32
    dst_ofs := Phi(dst_ofs_0, dst_ofs_1).u32

    If(dst_ofs, array_byte_len).EQ.Unlikely {
        Goto(:End)
    }
    eval("Store(dst_data, dst_ofs, Load(src_data, src_ofs).#{suffix}).#{suffix}")
    src_ofs_1 := Sub(src_ofs, elm_size).u32
    dst_ofs_1 := Add(dst_ofs, elm_size).u32
    Goto(:CopyLoop)

  Label(:End)
    Return(new_typed_array).ptr

  Label(:SlowPathEntrypoint)
    ep_offset = get_entrypoint_offset("#{macroType}_ARRAY_TO_REVERSED_SLOW_PATH")
    Intrinsic(:SLOW_PATH_ENTRY, typed_array).AddImm(ep_offset).MethodAsImm("#{type}ArrayToReversedOddSavedBridge").Terminator.ptr
    Intrinsic(:UNREACHABLE).Terminator.void if defines.DEBUG
  }
end

GenerateToReversed('Int8')
GenerateToReversed('Int16')
GenerateToReversed('Int32')
GenerateToReversed('BigInt64')
GenerateToReversed('Float32')
GenerateToReversed('Float64')
GenerateToReversed('Uint8')
GenerateToReversed('Uint16')
GenerateToReversed('Uint32')
GenerateToReversed('BigUint64')

scoped_macro(:clamp_u8_val) do |val0|
  If(val0, Cast(0).i32).LT.Unlikely {
      _val1 := 0
      Goto(:_Done)
  } 
  If(val0, Cast(255).i32).GT.Unlikely {
      _val2 := 255
      Goto(:_Done)
  } Else {
      _val0 := val0
  }

Label(:_Done)
  _val := Phi(_val1, _val2, _val0).u8
end

def GenerateTypedArrayWith(name, itype, etype, elm_size, mode)
  if mode == "signed"
    prefix = "TYPED_ARRAY"
  else
    prefix = "TYPED_UNSIGNED_ARRAY"
  end

  function("#{name}ArrayWithTlab".to_sym,
          params: {tarray: 'ref', pos: 'i32', val: "#{itype}"},
          regmap: $full_regmap,
          regalloc_set: $panda_mask,
          mode: [:FastPath]) {

    # Arm32 is not supported
    if Options.arch == :arm32
      Intrinsic(:UNREACHABLE).Terminator.void
      next
    end

    # Clamp u8 for ClampedArray
    if name == "Uint8Clamped"
      clamped_val := clamp_u8_val(val)
    end

    # pos < 0 (Out of range)
    If(pos, Cast(0).i32).LT.Unlikely {
      Goto(:SlowPathEntrypoint)
    }

    # pos >= length (Out of range)
    length := LoadI(tarray).Imm(eval("Constants::#{prefix}_LENGTH_INT_OFFSET")).i32
    If(pos, length).GE.Unlikely {
      Goto(:SlowPathEntrypoint)
    }

    # load TypedArray params, ArrayBuffer and Array
    if prefix == "TYPED_ARRAY"
      byte_offset := Cast(LoadI(tarray).Imm(Constants::TYPED_ARRAY_BYTE_OFFSET_OFFSET).f64).u32
    else
      byte_offset := LoadI(tarray).Imm(Constants::TYPED_UNSIGNED_ARRAY_BYTE_OFFSET_OFFSET).u32
    end
    array_byte_len := Mul(length, elm_size).u32
    buffer := LoadI(tarray).Imm(eval("Constants::#{prefix}_BUFFER_OFFSET")).ref
    packet := LoadI(buffer).Imm(Constants::ARRAY_BUFFER_DATA_OFFSET).ref
    If(packet, 0).EQ.Unlikely {
      Goto(:SlowPathEntrypoint)
    }

    # allocate new TypedArray, ArrayBuffer and Array
    pklass := LoadI(packet).Imm(Constants::OBJECT_CLASS_OFFSET).ref
    bklass := LoadI(buffer).Imm(Constants::OBJECT_CLASS_OFFSET).ref
    tklass := LoadI(tarray).Imm(Constants::OBJECT_CLASS_OFFSET).ref
    allocate_tarray_tlab(pklass, bklass, tklass, array_byte_len, eval("Constants::#{prefix}_CLASS_SIZE_WITH_ALIGNMENT"))

    # new Array
    StoreI(new_packet, array_byte_len).Imm(Constants::ARRAY_LENGTH_OFFSET).u32
    src_data := Add(AddI(packet).Imm(Constants::ARRAY_DATA_OFFSET).ptr, byte_offset).ptr
    dst_data := AddI(new_packet).Imm(Constants::ARRAY_DATA_OFFSET).ptr
    copy_u8_chars(src_data, dst_data, array_byte_len)

    # new ArrayBuffer
    StoreI(new_buffer, new_packet).Imm(Constants::ARRAY_BUFFER_DATA_OFFSET).ptr
    StoreI(new_buffer, dst_data).Imm(Constants::ARRAY_BUFFER_NATIVE_DATA_OFFSET).ptr
    StoreI(new_buffer, array_byte_len).Imm(Constants::ARRAY_BUFFER_BYTE_LENGTH_OFFSET).u32
    StoreI(new_buffer, Cast(0).u8).Imm(Constants::ARRAY_BUFFER_IS_RESIZABLE_OFFSET).u8

    # new TypedArray
    copy_u8_chars(tarray, new_tarray, eval("Constants::#{prefix}_CLASS_SIZE"))
    if prefix == "TYPED_ARRAY"
      StoreI(new_tarray, Cast(0).f64).Imm(Constants::TYPED_ARRAY_BYTE_OFFSET_OFFSET).f64
    else
      StoreI(new_tarray, Cast(0).u32).Imm(Constants::TYPED_UNSIGNED_ARRAY_BYTE_OFFSET_OFFSET).u32
    end
    StoreI(new_tarray, new_buffer).Imm(eval("Constants::#{prefix}_BUFFER_OFFSET")).ptr

    # set val on pos
    pos_offset := Mul(pos, elm_size).u32
    if name == "Uint8Clamped"
      eval("Store(dst_data, pos_offset, clamped_val).#{etype}")
    else
      eval("Store(dst_data, pos_offset, val).#{etype}")
    end
    Return(new_tarray).ptr

  Label(:SlowPathEntrypoint)
    entrypoint = get_entrypoint_offset("#{name.gsub(/(.)([A-Z])/,'\1_\2').upcase}_ARRAY_WITH_SLOW_PATH")
    Intrinsic(:SLOW_PATH_ENTRY, tarray, pos, val).AddImm(entrypoint).MethodAsImm("#{name}ArrayWithOddSavedBridge").Terminator.ptr
    Intrinsic(:UNREACHABLE).Terminator.void if defines.DEBUG
  }
end

GenerateTypedArrayWith('Int8', 'i8', 'i8', 1, "signed")
GenerateTypedArrayWith('Int16', 'i16', 'i16', 2, "signed")
GenerateTypedArrayWith('Int32', 'i32', 'i32', 4, "signed")
GenerateTypedArrayWith('BigInt64', 'i64', 'i64', 8, "signed")
GenerateTypedArrayWith('Float32', 'u32', 'u32', 4, "signed")
GenerateTypedArrayWith('Float64', 'u64', 'u64', 8, "signed")
GenerateTypedArrayWith('Uint8Clamped', 'i32', 'u8', 1, "unsigned")
GenerateTypedArrayWith('Uint8', 'i32', 'u8', 1, "unsigned")
GenerateTypedArrayWith('Uint16', 'i32', 'u16', 2, "unsigned")
GenerateTypedArrayWith('Uint32', 'i64', 'u32', 4, "unsigned")
GenerateTypedArrayWith('BigUint64', 'i64', 'u64', 8, "unsigned")
